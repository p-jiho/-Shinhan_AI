{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "afe6310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pytimekr import pytimekr\n",
    "from datetime import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0925f02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드 고정\n",
    "def seed_everything(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8ce3aa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence generation and multi-step target creation\n",
    "def generate_sequences(data, seq_length, n_steps):\n",
    "    sequences_x = []\n",
    "    sequences_y = []\n",
    "    \n",
    "    for i in range(len(data) - seq_length - n_steps + 1):\n",
    "        sequences_x.append(data[i:i+seq_length, :])\n",
    "        sequences_y.append(data[i+seq_length:i+seq_length+n_steps, 0]) # We predict 'CNY_KRW_Close'\n",
    "    \n",
    "    return np.array(sequences_x), np.array(sequences_y)\n",
    "\n",
    "# CNN model definition\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, kernel_size=3):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, hidden_dim, kernel_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.cnn(x)\n",
    "        x = x.mean(2)\n",
    "        x = self.fc(x)\n",
    "        return x.view(-1, self.output_dim)\n",
    "\n",
    "# Train function\n",
    "def train(model, train_loader, loss_function, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    return train_loss / len(train_loader)\n",
    "\n",
    "# MAPE 계산을 위한 함수 정의\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    epsilon = 1e-7  # 아주 작은 상수\n",
    "    return np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100\n",
    "\n",
    "\n",
    "def test(model, test_loader, loss_function):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    all_outputs = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            test_loss += loss_function(outputs, targets).item()\n",
    "            \n",
    "            all_outputs.append(outputs.detach().numpy())\n",
    "            all_targets.append(targets.detach().numpy())\n",
    "            \n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    \n",
    "    mse = mean_squared_error(all_targets, all_outputs)\n",
    "    mae = mean_absolute_error(all_targets, all_outputs)\n",
    "    mape = mean_absolute_percentage_error(all_targets, all_outputs)\n",
    "    \n",
    "    return test_loss / len(test_loader), mse, mae, mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6cb0ae4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(country):\n",
    "    data = pd.read_csv(r'C:\\Users\\whfhr\\Desktop\\신한 AI\\사용data\\Total.csv')\n",
    "    data = data[sum(columns[country], [])]\n",
    "\n",
    "    seq_length = 30 # 입력 시퀀스\n",
    "    n_steps = 60 # 출력 시퀀스 n_steps의 스텝을 에측.\n",
    "\n",
    "    # 원본 데이터에서 끝의 n_steps개 데이터를 제외한 데이터에 대한 작업(data leakage를 완전 방지기 위함)\n",
    "    \n",
    "    source_data = data\n",
    "    data = source_data.head(len(data) - n_steps)\n",
    "\n",
    "    # Data scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "    X, y = generate_sequences(scaled_data, seq_length, n_steps)\n",
    "\n",
    "    train_size = int(len(X) * 0.8)\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "    # PyTorch tensors and DataLoader\n",
    "    train_dataset = TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train))\n",
    "    test_dataset = TensorDataset(torch.Tensor(X_test), torch.Tensor(y_test))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=128)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128)\n",
    "    \n",
    "    seed_everything(42)\n",
    "    \n",
    "    input_dim = X_train.shape[2]\n",
    "    hidden_dim = 64\n",
    "    output_dim = y_train.shape[1]\n",
    "    kernel_size = 3\n",
    "\n",
    "    model = CNNModel(input_dim, hidden_dim, output_dim, kernel_size)\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    n_epochs = 20\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = train(model, train_loader, loss_function, optimizer)\n",
    "        test_metrics = test(model, test_loader, loss_function)  # test_metrics는 이제 튜플입니다.\n",
    "        test_loss = test_metrics[0]  # test_loss는 튜플의 첫 번째 요소입니다.\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "    test_loss, mse, mae, mape = test(model, test_loader, loss_function)\n",
    "\n",
    "    # Prepare the last n_steps sequence from the test data\n",
    "    last_sequence = pd.DataFrame(scaled_data).tail(seq_length).values\n",
    "\n",
    "    # Predict the next n_steps\n",
    "    next_n_steps_predictions = np.empty((n_steps, 1))\n",
    "    for i in range(n_steps):\n",
    "        next_step_prediction = model(torch.Tensor(last_sequence).unsqueeze(0)).detach().numpy()[:, 0]  # skip across feature dimensions\n",
    "        next_n_steps_predictions[i, :] = next_step_prediction.reshape(-1, 1)\n",
    "\n",
    "        # Change - Update last_sequence correctly considering dimensions\n",
    "        last_sequence = np.roll(last_sequence, shift=-1, axis=0)\n",
    "        last_sequence[-1, 0] = next_step_prediction\n",
    "\n",
    "    # Inverse transformation\n",
    "    next_n_steps_predictions_inverse = scaler.inverse_transform(\n",
    "        np.hstack((next_n_steps_predictions, np.zeros((next_n_steps_predictions.shape[0], data.shape[1] - next_n_steps_predictions.shape[1]))))\n",
    "    )[:, 0]\n",
    "    return source_data[columns[country][0]].tail(n_steps+1).iloc[0], next_n_steps_predictions_inverse\n",
    "\n",
    "def prediction_date_func():\n",
    "    n_steps = 60\n",
    "    start = datetime.today().replace(month = 1, day=1)\n",
    "    end = start + relativedelta(years=1)\n",
    "    date = pd.date_range(start, periods=(end-start).days+1)\n",
    "\n",
    "    week = []\n",
    "    for i in date:\n",
    "        if i.weekday() == 6 or i.weekday() == 5:\n",
    "            week.append(i)\n",
    "        else: pass\n",
    "    \n",
    "    holidays = pytimekr.holidays()\n",
    "\n",
    "    date = list(map(lambda x: x.strftime(\"%Y-%m-%d\"), date))\n",
    "    week = list(map(lambda x: x.strftime(\"%Y-%m-%d\"), week))\n",
    "    holidays = list(map(lambda x: x.strftime(\"%Y-%m-%d\"), holidays))\n",
    "\n",
    "    date = list(set(date) - set(week))\n",
    "    date = list(set(date) - set(holidays))\n",
    "    date.sort()\n",
    "\n",
    "    today = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    if today in date:\n",
    "        prediction_date = date[date.index(today)+1:date.index(today)+n_steps+1]\n",
    "    else:\n",
    "        for i in date:\n",
    "            if i > today:\n",
    "                prediction_date = date[date.index(i):date.index(i)+n_steps]\n",
    "                break\n",
    "            else: pass\n",
    "    return prediction_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2a1754ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_date(country):\n",
    "    _, next_n_steps_predictions_inverse = prediction(country)\n",
    "    prediction_date = prediction_date_func()\n",
    "            \n",
    "    next_n_steps_predictions_inverse = list(next_n_steps_predictions_inverse)\n",
    "    minidx = next_n_steps_predictions_inverse.index(min(next_n_steps_predictions_inverse))\n",
    "    \n",
    "    return prediction_date[minidx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "987532bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = {\"미국\" : [['USD_KRW_Close'],['Korea_KOSDAQ_Close']],\n",
    "          \"일본\" : [['JPY_KRW_Close'],['Korea_KOSDAQ_Close', 'Korea_KOSPI200_Close']],\n",
    "          \"영국\" : [['GBP_KRW_Close'],[\"USA_DOW_Close\"]],\n",
    "          \"유로존\" : [['EUR_KRW_Close'],[\"USA_DOW_Close\"]],\n",
    "          \"중국\" : [['CNY_KRW_Close'],[\"USA_DOW_Close\"]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a087ffbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(오늘이 2022년 )\n",
      "2023-05-15 ~ 2023-08-08 기간 내에 지정하신 국가의 환율이 낮은 날을 추천을 해드립니다.\n",
      "미국, 일본, 영국, 유로존, 중국 중 여행을 떠날 나라를 지정해주세요.\n",
      "미국\n",
      "\n",
      "(개장일 기준) 2023-05-15 ~ 2023-08-08 중 미국 환율이 낮은 날은 \"2023-08-08(+-2일)\"부근입니다.\n"
     ]
    }
   ],
   "source": [
    "def recommend_date_result():\n",
    "    prediction_date = prediction_date_func()\n",
    "    print(\"(오늘이 2022년 )\")\n",
    "    print(\"{} ~ {} 기간 내에 지정하신 국가의 환율이 낮은 날을 추천을 해드립니다.\".format(prediction_date[0],prediction_date[-1]))\n",
    "    print(\"미국, 일본, 영국, 유로존, 중국 중 여행을 떠날 나라를 지정해주세요.\")\n",
    "    country = input()\n",
    "    \n",
    "    if country in [\"미국\", \"일본\", \"영국\", \"유로존\", \"중국\"]:\n",
    "        date = recommend_date(country)\n",
    "        print(\"\\n(개장일 기준) {} ~ {} 중 {} 환율이 낮은 날은 \\\"{}(+-2일)\\\"부근입니다.\".format(prediction_date[0],prediction_date[-1],country,date))\n",
    "    else:\n",
    "        print(\"\\n국가를 다시 확인해주세요.\\n\")\n",
    "        recommend_date_result()\n",
    "recommend_date_result()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2fb57c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_country():\n",
    "    today_usd, USD = list(prediction(\"미국\"))\n",
    "    today_jpy, JPY = list(prediction(\"일본\"))\n",
    "    today_gbp, GBP = list(prediction(\"영국\"))\n",
    "    today_eur, EUR = list(prediction(\"유로존\"))\n",
    "    today_cny, CNY = list(prediction(\"중국\"))\n",
    "    \n",
    "    prediction_date = prediction_date_func()\n",
    "    \n",
    "    enc_usd = (list(USD)/today_usd.values*100)\n",
    "    enc_jpy = (list(JPY)/today_jpy.values*100)\n",
    "    enc_gbp = (list(GBP)/today_gbp.values*100)\n",
    "    enc_eur = (list(EUR)/today_eur.values*100)\n",
    "    enc_cny = (list(CNY)/today_cny.values*100)\n",
    "    \n",
    "    min_value = [min(enc_usd),min(enc_jpy),min(enc_gbp),min(enc_eur),min(enc_cny)]\n",
    "    min_idx = min_value.index(min(min_value))\n",
    "    enchage = [enc_usd, enc_jpy, enc_gbp, enc_eur, enc_cny]\n",
    "    \n",
    "    idx = list(enchage[min_idx]).index(min(enchage[min_idx]))\n",
    "    \n",
    "    country = [\"미국\", \"일본\", \"영국\", \"유로존\", \"중국\"]\n",
    "    \n",
    "    return country[min_idx], prediction_date[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "bf402543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "미국, 일본, 영국, 유로존, 중국 중 추천을 해드립니다.\n",
      "\n",
      "현재 대비 지정일에 환율이 가장 낮은 나라는 미국 이고, 2023-08-08 날 입니다.\n"
     ]
    }
   ],
   "source": [
    "print(\"미국, 일본, 영국, 유로존, 중국 중 추천을 해드립니다.\")\n",
    "def recommend_country_result():\n",
    "    prediction_date = prediction_date_func()\n",
    "\n",
    "    country, date = recommend_country()\n",
    "    print(\"\\n현재 대비 지정일에 환율이 가장 낮은 나라는 {} 이고, {} 날 입니다.\".format(country, date))\n",
    "    \n",
    "recommend_country_result()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e4868d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
